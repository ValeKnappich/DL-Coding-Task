{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DL Coding Task This repository contains my solution to the deep learning coding task from Prof. Vu. Scripts config.py : File that contains constants like UNIQUE_INTENTS and mappings to IDs. Also contains hyperparameters. model.py : Defines the model as PyTorch Lightning Module and therefore implements the model architecture, as well as training and validation procedure. Model is saved after training. train.py : Used to train the model with a pl.Trainer. Uses multiple GPU's if available and performs early stopping. data.py : Defines the pl.LightningDataModule, that implements data loading and preprocessing. Executing the script will run and end-to-end test, weather the conversion between formats worked correctly for the training data. test.py : Used to perform inference on the dev set. Usage python train.py # train the model, checkpoint is saved to disk python test.py # loads the dumped model, predicts the classes and outputs the results to disk","title":"Home"},{"location":"#dl-coding-task","text":"This repository contains my solution to the deep learning coding task from Prof. Vu.","title":"DL Coding Task"},{"location":"#scripts","text":"config.py : File that contains constants like UNIQUE_INTENTS and mappings to IDs. Also contains hyperparameters. model.py : Defines the model as PyTorch Lightning Module and therefore implements the model architecture, as well as training and validation procedure. Model is saved after training. train.py : Used to train the model with a pl.Trainer. Uses multiple GPU's if available and performs early stopping. data.py : Defines the pl.LightningDataModule, that implements data loading and preprocessing. Executing the script will run and end-to-end test, weather the conversion between formats worked correctly for the training data. test.py : Used to perform inference on the dev set.","title":"Scripts"},{"location":"#usage","text":"python train.py # train the model, checkpoint is saved to disk python test.py # loads the dumped model, predicts the classes and outputs the results to disk","title":"Usage"},{"location":"approach/","text":"Approach","title":"Approach Overview"},{"location":"approach/#approach","text":"","title":"Approach"},{"location":"data-api-reference/","text":"data NLUDataSet Objects class NLUDataSet(Dataset) Generic Dataset, that holds a dict with columns __init__ | __init__(data: dict) NLUDataset constructor. Converts the columns to tensors of type long. Arguments : data dict - Dict holding the columns. DataModule Objects class DataModule(pl.LightningDataModule) Data Module to load data from disk and create DataLoaders __init__ | __init__(mode: str = \"fit\", no_split: bool = False) DataModule constructor. Loads config from config.py and binds them to the object. Calls the setup method to load data for the given mode (see below). Arguments : mode str, optional - Specify to load either training or testing data (\"fit\" or \"test\"). Defaults to \"fit\". no_split bool, optional - Flag to not perform a random split. Defaults to False. load_and_clean | load_and_clean() -> dict Load data from disk and remove characters that are not supported by the tokenizer Returns : data dict - dict containing the columns as long tensors setup | setup(mode: str) Load data from disk and create NLUDataSets. Uses the pretrained tokenizer (according to config.config.model_name), uses padding and truncation to get a constant sequence length (config.config.sequence_length). Arguments : mode str - Specify to load either training or testing data (\"fit\" or \"test\"). Raises : ValueError - If mode is not \"fit\" or \"test\". train_dataloader | train_dataloader() -> DataLoader Create DataLoader from train set. Returns : DataLoader - Training Loader val_dataloader | val_dataloader() -> DataLoader Create DataLoader from validation set. Returns : DataLoader - Validation Loader test_dataloader | test_dataloader() -> DataLoader Create DataLoader from test set. Returns : DataLoader - Test Loader format2IOB | format2IOB(tokens_matrix: torch.Tensor) -> torch.Tensor Convert the format from json containig entity spans to IOB format. Arguments : tokens_matrix torch.Tensor - Tokens created by the tokenizer of shape (num_examples, sequence_length) Returns : torch.Tensor - Token labels of shape (num_examples, sequence_length) format2original | format2original(intents: torch.Tensor, token_labels_matrix: torch.Tensor) -> dict Format back to the original JSON format. Arguments : intents torch.Tensor - Tensor of the predicted intent-ID's token_labels_matrix torch.Tensor - Tensor of the tokens Returns : dict - results in the original JSON format","title":"Data API Reference"},{"location":"data-api-reference/#data","text":"","title":"data"},{"location":"data-api-reference/#nludataset-objects","text":"class NLUDataSet(Dataset) Generic Dataset, that holds a dict with columns","title":"NLUDataSet Objects"},{"location":"data-api-reference/#__init__","text":"| __init__(data: dict) NLUDataset constructor. Converts the columns to tensors of type long. Arguments : data dict - Dict holding the columns.","title":"__init__"},{"location":"data-api-reference/#datamodule-objects","text":"class DataModule(pl.LightningDataModule) Data Module to load data from disk and create DataLoaders","title":"DataModule Objects"},{"location":"data-api-reference/#__init___1","text":"| __init__(mode: str = \"fit\", no_split: bool = False) DataModule constructor. Loads config from config.py and binds them to the object. Calls the setup method to load data for the given mode (see below). Arguments : mode str, optional - Specify to load either training or testing data (\"fit\" or \"test\"). Defaults to \"fit\". no_split bool, optional - Flag to not perform a random split. Defaults to False.","title":"__init__"},{"location":"data-api-reference/#load_and_clean","text":"| load_and_clean() -> dict Load data from disk and remove characters that are not supported by the tokenizer Returns : data dict - dict containing the columns as long tensors","title":"load_and_clean"},{"location":"data-api-reference/#setup","text":"| setup(mode: str) Load data from disk and create NLUDataSets. Uses the pretrained tokenizer (according to config.config.model_name), uses padding and truncation to get a constant sequence length (config.config.sequence_length). Arguments : mode str - Specify to load either training or testing data (\"fit\" or \"test\"). Raises : ValueError - If mode is not \"fit\" or \"test\".","title":"setup"},{"location":"data-api-reference/#train_dataloader","text":"| train_dataloader() -> DataLoader Create DataLoader from train set. Returns : DataLoader - Training Loader","title":"train_dataloader"},{"location":"data-api-reference/#val_dataloader","text":"| val_dataloader() -> DataLoader Create DataLoader from validation set. Returns : DataLoader - Validation Loader","title":"val_dataloader"},{"location":"data-api-reference/#test_dataloader","text":"| test_dataloader() -> DataLoader Create DataLoader from test set. Returns : DataLoader - Test Loader","title":"test_dataloader"},{"location":"data-api-reference/#format2iob","text":"| format2IOB(tokens_matrix: torch.Tensor) -> torch.Tensor Convert the format from json containig entity spans to IOB format. Arguments : tokens_matrix torch.Tensor - Tokens created by the tokenizer of shape (num_examples, sequence_length) Returns : torch.Tensor - Token labels of shape (num_examples, sequence_length)","title":"format2IOB"},{"location":"data-api-reference/#format2original","text":"| format2original(intents: torch.Tensor, token_labels_matrix: torch.Tensor) -> dict Format back to the original JSON format. Arguments : intents torch.Tensor - Tensor of the predicted intent-ID's token_labels_matrix torch.Tensor - Tensor of the tokens Returns : dict - results in the original JSON format","title":"format2original"},{"location":"model-api-reference/","text":"model IntentAndEntityModel Objects class IntentAndEntityModel(pl.LightningModule) __init__ | __init__() IntentAndEntityModel constructor. Loads the config from config.py and binds it to the object. Create Layers, including pretrained, as specified in config.config.model_name forward | forward(input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor] Forward pass Arguments : input_ids torch.Tensor - Input ID's created by the tokenizer of shape (batch_size, sequence_length) attention_mask torch.Tensor - Attention masks created by the tokenizer of shape (batch_size, sequence_length) Returns : intent_logits, ner_logits (Tuple[torch.Tensor, torch.Tensor]): Logits of the intent and NER heads before Softmax configure_optimizers | configure_optimizers() -> Tuple[ | List[torch.optim.Optimizer], List[torch.optim.lr_scheduler._LRScheduler] | ] Configure optimizer and lr scheduler Returns : Tuple[List[torch.optim.Optimizer], List[torch.optim.lr_scheduler._LRScheduler]]: Optimizer and LR Scheduler loss | loss(intent_logits: torch.Tensor, ner_logits: torch.Tensor, intent_labels: torch.Tensor, ner_labels: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor] Calculate the individual losses for intent and ner respectively Arguments : intent_logits torch.Tensor - Intent logits before SoftMax ner_logits torch.Tensor - NER logits before SoftMax intent_labels torch.Tensor - Predicted intent-ID's ner_labels torch.Tensor - Predicted token labels Returns : Tuple[torch.Tensor, torch.Tensor]: Intent loss and NER loss accuracy | accuracy(logits: torch.Tensor, labels: torch.Tensor) -> float Calculate accuracy for logits and labels Arguments : logits torch.Tensor - Logits labels torch.Tensor - True labels Returns : float - Accuracy training_step | training_step(batch: dict, batch_idx: int) -> torch.Tensor Training step Arguments : batch dict - Dict containing the columns of the batch batch_idx int - Batch index Returns : torch.Tensor - Combined loss as mean between intent and NER loss validation_step | validation_step(batch: dict, batch_idx: int) Validation step Arguments : batch dict - Dict containing the columns of the batch batch_idx int - Batch index","title":"Model API Reference"},{"location":"model-api-reference/#model","text":"","title":"model"},{"location":"model-api-reference/#intentandentitymodel-objects","text":"class IntentAndEntityModel(pl.LightningModule)","title":"IntentAndEntityModel Objects"},{"location":"model-api-reference/#__init__","text":"| __init__() IntentAndEntityModel constructor. Loads the config from config.py and binds it to the object. Create Layers, including pretrained, as specified in config.config.model_name","title":"__init__"},{"location":"model-api-reference/#forward","text":"| forward(input_ids: torch.Tensor, attention_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor] Forward pass Arguments : input_ids torch.Tensor - Input ID's created by the tokenizer of shape (batch_size, sequence_length) attention_mask torch.Tensor - Attention masks created by the tokenizer of shape (batch_size, sequence_length) Returns : intent_logits, ner_logits (Tuple[torch.Tensor, torch.Tensor]): Logits of the intent and NER heads before Softmax","title":"forward"},{"location":"model-api-reference/#configure_optimizers","text":"| configure_optimizers() -> Tuple[ | List[torch.optim.Optimizer], List[torch.optim.lr_scheduler._LRScheduler] | ] Configure optimizer and lr scheduler Returns : Tuple[List[torch.optim.Optimizer], List[torch.optim.lr_scheduler._LRScheduler]]: Optimizer and LR Scheduler","title":"configure_optimizers"},{"location":"model-api-reference/#loss","text":"| loss(intent_logits: torch.Tensor, ner_logits: torch.Tensor, intent_labels: torch.Tensor, ner_labels: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor] Calculate the individual losses for intent and ner respectively Arguments : intent_logits torch.Tensor - Intent logits before SoftMax ner_logits torch.Tensor - NER logits before SoftMax intent_labels torch.Tensor - Predicted intent-ID's ner_labels torch.Tensor - Predicted token labels Returns : Tuple[torch.Tensor, torch.Tensor]: Intent loss and NER loss","title":"loss"},{"location":"model-api-reference/#accuracy","text":"| accuracy(logits: torch.Tensor, labels: torch.Tensor) -> float Calculate accuracy for logits and labels Arguments : logits torch.Tensor - Logits labels torch.Tensor - True labels Returns : float - Accuracy","title":"accuracy"},{"location":"model-api-reference/#training_step","text":"| training_step(batch: dict, batch_idx: int) -> torch.Tensor Training step Arguments : batch dict - Dict containing the columns of the batch batch_idx int - Batch index Returns : torch.Tensor - Combined loss as mean between intent and NER loss","title":"training_step"},{"location":"model-api-reference/#validation_step","text":"| validation_step(batch: dict, batch_idx: int) Validation step Arguments : batch dict - Dict containing the columns of the batch batch_idx int - Batch index","title":"validation_step"},{"location":"results/","text":"Results","title":"Results"},{"location":"results/#results","text":"","title":"Results"},{"location":"task/","text":"Task Overview The task is to predict the intent and entities in user utterances. Data Format Labelled training data and unlabelled test data. { \"0\": { \"intent\": \"AddToPlaylist\", \"text\": \"Add a tune to my elrow Guest List\", \"slots\": { \"music_item\": \"tune\", \"playlist_owner\": \"my\", \"playlist\": \"elrow Guest List\" }, \"positions\": { \"music_item\": [ 6, 9 ], \"playlist_owner\": [ 14, 15 ], \"playlist\": [ 17, 32 ] } } }","title":"Task Overview"},{"location":"task/#task-overview","text":"The task is to predict the intent and entities in user utterances.","title":"Task Overview"},{"location":"task/#data-format","text":"Labelled training data and unlabelled test data. { \"0\": { \"intent\": \"AddToPlaylist\", \"text\": \"Add a tune to my elrow Guest List\", \"slots\": { \"music_item\": \"tune\", \"playlist_owner\": \"my\", \"playlist\": \"elrow Guest List\" }, \"positions\": { \"music_item\": [ 6, 9 ], \"playlist_owner\": [ 14, 15 ], \"playlist\": [ 17, 32 ] } } }","title":"Data Format"}]}